{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Introduction to Statistical Programming in Python</center></h1>\n",
    "\n",
    "You will be running Python code inside of a Jupyter notebook and so all code must be written in code blocks which look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run code in a code block, you must select the code block and press the run button at the top, or simply highlight the block and press `Ctrl+Enter`.\n",
    "\n",
    "You can also save your progress at any point by going to __File --> Save and Checkpoint__.\n",
    "\n",
    "As you work through this practical, you may also be presented with error codes (some are intentional, others are related to different versions, we will solve them as we encounter them in different ways). These are shown in the <span style=\"color: red\">red boxes</span> and will help you figure out what has gone wrong. \n",
    "\n",
    "Here are some further useful tips and shortcuts for optimising your time with JNs: https://towardsdatascience.com/jypyter-notebook-shortcuts-bf0101a98330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.graphics.gofplots\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, ttest_1samp, shapiro, chi2_contingency, linregress, kstest, anderson\n",
    "from statsmodels.api import OLS, add_constant\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from packaging import version\n",
    "\n",
    "if version.parse(np.__version__)>=version.parse('1.19.2'):\n",
    "    print('numpy is up to date')\n",
    "else:\n",
    "    print('update numpy, current version:')\n",
    "    print(np.__version__)\n",
    "if version.parse(pd.__version__)>=version.parse('1.2.2'):\n",
    "    print('pandas is up to date')\n",
    "else:\n",
    "    print('update pandas, current version:')\n",
    "    print(pd.__version__)\n",
    "if version.parse(scipy.__version__)>=version.parse('1.6.0'):\n",
    "    print('scipy is up to date')\n",
    "else:\n",
    "    print('update scipy, current version:')\n",
    "    print(scipy.__version__)\n",
    "if version.parse(mpl.__version__)>=version.parse('3.3.4'):\n",
    "    print('matplotlib is up to date')\n",
    "else:\n",
    "    print('update matplotlib, current version:')\n",
    "    print(mpl.__version__)\n",
    "if version.parse(sns.__version__)>=version.parse('0.11.1'):\n",
    "    print('seaborn is up to date')\n",
    "else:\n",
    "    print('update seaborn, current version:')\n",
    "    print(sns.__version__)\n",
    "if version.parse(statsmodels.__version__)>=version.parse('0.12.1'):\n",
    "    print('statsmodels is up to date')\n",
    "else:\n",
    "    print('update statsmodels, current version:')\n",
    "    print(statsmodels.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 1</h3>\n",
    "\n",
    "Is the overall health index normally distributed or not across local authorities in England in 2015? (dataset:  `healthIndexEngland.xlsx`,  sheet: _healthIndex2015LocalAuthority_)\n",
    "\n",
    "Documentation: [Shapiro-Wilk](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html), [Kolmogorov-Smirnov](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html) (hint: you need another input here) and [Anderson-Darling](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html) (hint: the output is different here) tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_2015 = pd.read_excel('healthIndexEngland.xlsx', sheet_name='healthIndex2015LocalAuthority')\n",
    "plt.figure()\n",
    "sns.histplot(hi_2015.healthIndex2015)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(hi_2015.healthIndex2015, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a different test to do, see the documentation (click links above) about the inputs and focus on one-tailed tests and change the below.\n",
    "\n",
    "If you then still need help, use your favourite LLM to suggest code. E.g. prompt it with:\n",
    "\n",
    "_Given the following code, and data contained in hi_2015.healthIndex2015, can you rewrite this for the Kolomogorov-Smirnov test from SciPy. Can you then explain the parts of the original code that are different?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for normality by using the Shapiro-Wilk test\n",
    "stat, pval = shapiro(hi_2015.healthIndex2015)\n",
    "print('stat=%.3f, p-value=%.3f\\n' % (stat,pval))\n",
    "if pval > 0.05:\n",
    "    print('The data are normally distributed')\n",
    "else:\n",
    "    print('The data are not normally distributed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 2</h3>\n",
    "\n",
    "Use appropriate statistical comparison test to comment on how the health indices per domain across the regions have changed from 2015 compared with 2018, has it gotten better, worse or stayed the same? \n",
    "(dataset:  `healthIndexEngland.xlsx`,  sheet: _healthIndexPerDomainRegions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_domain_region = pd.read_excel('healthIndexEngland.xlsx', sheet_name='healthIndexPerDomainRegions')\n",
    "hi_domain_region_2015 = hi_domain_region.healthIndexOverall2015 \n",
    "hi_domain_region_2018 = hi_domain_region['healthIndexOverall2018'] \n",
    "\n",
    "# test for normality by plotting the two distributions\n",
    "# note that this function will be removed in a future version of seaborn\n",
    "plt.figure()\n",
    "#sns.distplot(hi_domain_region_2015, hist=False)\n",
    "#sns.distplot(hi_domain_region_2018, bins=10, hist=True)\n",
    "\n",
    "sns.displot(hi_domain_region_2015, kind='hist', kde=True)\n",
    "sns.displot(hi_domain_region_2018, kind='hist', kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the graph above we see that the data is normally distributed,  \n",
    "# we can therefore use a parametric test to assess significance \n",
    "\n",
    "print(ttest_rel(hi_domain_region_2015, hi_domain_region_2018))\n",
    "# using a paired t-test here because health index is a repeated measure in 2015 and 2018 \n",
    "print('if you want to print just the p-value:', \n",
    "      ttest_rel(hi_domain_region_2015, hi_domain_region_2018)[1]) \n",
    "\n",
    "# let's compare the means now to look at differences \n",
    "if np.mean(hi_domain_region_2015) > np.mean(hi_domain_region_2018):\n",
    "    print('the health index dropped in 2018')\n",
    "elif np.mean(hi_domain_region_2015) == np.mean(hi_domain_region_2018):\n",
    "    print('the health index has remained the same in 2015 and 2018')\n",
    "else:\n",
    "    print('the health index increased in 2018')\n",
    "    \n",
    "print(np.mean(hi_domain_region_2015))\n",
    "print(np.mean(hi_domain_region_2018))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extra Assignment A & B</h3>\n",
    "\n",
    "Do the same as assignment 2 for local authorities, does the answer stay the same?\n",
    "(dataset:  `healthIndexEngland.xlsx`,  sheet: _healthIndexPerDomainAuthority_)\n",
    "\n",
    "Do the same as assignment 2 except now use data from all 4 years (2015 to 2018), is there a significant difference between any of the 4 years? (dataset:  `healthIndexEngland.xlsx`,  sheet: _healthIndexPerDomainRegions_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_domain_region = pd.read_excel('healthIndexEngland.xlsx', sheet_name='healthIndexPerDomainRegions')\n",
    "\n",
    "# your code goes here - try to first use the above code and rewrite it.\n",
    "# once you know the data format you can then prompt an LLM if easier (always try yourself first, and ask it to explain the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 3</h3>\n",
    "\n",
    "Visualize deaths per week for each year, what do you observe?\n",
    "(dataset:  `weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx`,  sheet: _weeklyDeaths_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx')\n",
    "deaths_object = pd.ExcelFile('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx')\n",
    "print(deaths_object.sheet_names)\n",
    "\n",
    "ee = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx', sheet_name='East England')\n",
    "em = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx', sheet_name='East Midlands')\n",
    "l = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx', sheet_name='London')\n",
    "nee = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx', sheet_name='North East England')\n",
    "nwe = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx', sheet_name='North West England')\n",
    "see = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx', sheet_name='South East England')\n",
    "swe = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx', sheet_name='South West England')\n",
    "w = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx', sheet_name='Wales')\n",
    "wm = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx', sheet_name='West Midlands')\n",
    "yth = pd.read_excel('weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx', sheet_name='Yorkshire and The Humber')\n",
    "\n",
    "\n",
    "ew = ee.add(em).add(l).add(nee).add(nwe).add(see).add(swe).add(w).add(wm).add(yth)\n",
    "ew.year = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "ew = ew.drop(['week'], axis=1)\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "for i in range(len(ew.year.values)):\n",
    "    sns.lineplot(x=ew.columns[1:], y=ew.drop(['year'], axis=1).iloc[i], label=ew.iloc[i].year)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('weeks')\n",
    "plt.ylabel('deaths')\n",
    "ew.iloc[:,10:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe for the COVID years with complete data? Any differences between these?\n",
    "\n",
    "Do you see any other patterns over the years?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 4</h3>\n",
    "\n",
    "Are the data normally distributed?\n",
    "\n",
    "Apply the appropriate test to show which weeks had higher deaths in 2020 (if any), what do you observe with respect to government restrictions implemented in the UK?\n",
    "(dataset: `weeklyRegisteredDeathsPerRegionPlusCovidCause.xlsx`,  sheet: _weeklyDeaths_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for normality first and choose between parametric and non-parametric tests\n",
    "\n",
    "deaths_ew_2020 = ew.loc[ew['year'] == 2020].values.tolist() # this creates a nested list with the year value in as well\n",
    "print(deaths_ew_2020)\n",
    "deaths_ew_2020 = deaths_ew_2020[0][1:] # taking just the week values after indexing within the list\n",
    "print(deaths_ew_2020) # if you print you can see the differences between the first and second list\n",
    "\n",
    "pvals_deaths_2020 = [] # we will use this list later in multiple correction\n",
    "for i in deaths_ew_2020:\n",
    "    pval = ttest_1samp(deaths_ew_2020, popmean=i, alternative='less').pvalue\n",
    "    pvals_deaths_2020.append(pval)\n",
    "\n",
    "for week, pvalue in enumerate(pvals_deaths_2020):\n",
    "    if pvalue <= 0.05:\n",
    "        print(week+1, pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the same as above for 2021\n",
    "deaths_ew_2021 = ew.loc[ew['year'] == 2021].values.tolist()\n",
    "\n",
    "# your code goes here\n",
    "print(deaths_ew_2021)\n",
    "deaths_ew_2021 = deaths_ew_2021[0][1:] # taking just the week values after indexing within the list\n",
    "print(deaths_ew_2021) # if you print you can see the differences between the first and second list\n",
    "\n",
    "pvals_deaths_2021 = [] # we will use this list later in multiple correction\n",
    "for i in deaths_ew_2021:\n",
    "    pval = ttest_1samp(deaths_ew_2021, popmean=i, alternative='less').pvalue\n",
    "    pvals_deaths_2021.append(pval)\n",
    "\n",
    "for week, pvalue in enumerate(pvals_deaths_2021):\n",
    "    if pvalue <= 0.05:\n",
    "        print(week+1, pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 5</h3>\n",
    "\n",
    "Visualize deaths due to respiratory causes per week for each year, what do you observe? Consider both observations across all years as well as COVID-19 specific years. \n",
    "<br>(dataset: `weeklyRegisteredDeathsEnglandWalesPlusCauses_v2021.xlsx`,  sheet: _respiratoryDiseasesAsCause_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_resp_ew = pd.read_excel('weeklyRegisteredDeathsEnglandWalesPlusCauses_v2021.xlsx', sheet_name='respiratoryDiseasesAsCause')\n",
    "deaths_resp_ew = deaths_resp_ew.drop(['week'], axis=1)\n",
    "\n",
    "sns.set_context(context='notebook', font_scale=1.3)\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('Paired')\n",
    "plt.figure(figsize=(14,8))\n",
    "for i in range(len(ew.year.values)):\n",
    "    sns.lineplot(x=deaths_resp_ew.columns[1:], y=deaths_resp_ew.drop(['year'], axis=1).iloc[i], label=ew.iloc[i].year)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('weeks')\n",
    "plt.ylabel('deaths')\n",
    "deaths_resp_ew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 6</h3>\n",
    "\n",
    "Apply appropriate test to show which weeks had higher respiratory related deaths in 2020 (if any), what do you observe?\n",
    "<br>(dataset: `weeklyRegisteredDeathsEnglandWalesPlusCauses.xlsx`,  sheet: _respiratoryDiseasesAsCause_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_resp_ew_2020 = deaths_resp_ew.iloc[10].values[1:].tolist()\n",
    "pvals_deaths_resp_2020 = []\n",
    "for i in deaths_resp_ew_2020:\n",
    "    pval = ttest_1samp(deaths_resp_ew_2020, popmean=i, alternative='less').pvalue\n",
    "    pvals_deaths_resp_2020.append(pval)\n",
    "\n",
    "for week, pvalue in enumerate(pvals_deaths_resp_2020):\n",
    "    if pvalue <= 0.05:\n",
    "        print(week+1, pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat this for 2021\n",
    "\n",
    "deaths_resp_ew_2021 = deaths_resp_ew.iloc[11].values[1:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extra Assignment C & D</h3>\n",
    "\n",
    "What if you add COVID-19 deaths in 2020 to respiratory causes for England+Wales, what do you observe? \n",
    "<br>(dataset: `weeklyRegisteredDeathsEnglandWalesPlusCauses_v2021.xlsx`, sheets: _respiratoryDiseasesAsCause_ and _covidAsCause_)\n",
    "\n",
    "What if you subtract COVID-19 deaths from total weekly deaths, what do you observe? \n",
    "<br>(dataset: `weeklyRegisteredDeathsPerRegionPlusCovidCause_v2021.xlsx`,  sheet: _weeklyDeaths_ and _covidAsCause_)\n",
    "\n",
    "Now repeat this for 2021 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 7</h3>\n",
    "\n",
    "Apply [multiple testing correction](https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html) to results from assignments 4 and 6, do the answers change? \n",
    "<br>(dataset: `weeklyRegisteredDeathsEnglandWalesPlusCauses_v2021.xlsx`,  sheets: _weeklyDeaths_ and _respiratoryDiseasesAsCause_)\n",
    "\n",
    "Also do this for 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(multipletests(pvals_deaths_2020, alpha=0.05, method='sidak'))\n",
    "corr_pvals_deaths_2020 = multipletests(pvals_deaths_2020, alpha=0.05, method='hommel')[1]\n",
    "for week, pvalue in enumerate(corr_pvals_deaths_2020):\n",
    "    if pvalue <= 0.05:\n",
    "        print(week+1, pvalue)\n",
    "\n",
    "corr_pvals_deaths_resp_2020 = multipletests(pvals_deaths_resp_2020, alpha=0.05, method='sidak')[1]\n",
    "# try with different FWER methods (see link above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat these above for 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extra Assignment E & F</h3>\n",
    "\n",
    "What if you apply different multiple testing procedures, (how) do the answers change?\n",
    "<br>(dataset: `weeklyRegisteredDeathsEnglandWalesPlusCauses.xlsx`,  sheets: _weeklyDeaths_ and _respiratoryDiseasesAsCause_)\n",
    "\n",
    "Looking back at extra assignment B, if there are differences between any of the 4 years, how would you go about finding out which one(s) is/are different? \n",
    "(dataset:  `healthIndexEngland.xlsx`,  sheet: _healthIndexPerDomainRegions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 8</h3>\n",
    "\n",
    "Apply [$χ^2$ test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html) to find if there was a difference between regions (London vs other) in COVID-19 vs other causes as registered cause of death over all of 2020?\n",
    "<br>(dataset: `weeklyRegisteredDeathsByLocationOtherCausesPlusCOVID.xlsx`,  sheet: _registeredDeathsRegions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_region = pd.read_excel('weeklyRegisteredDeathsByLocationOtherCausesPlusCOVID.xlsx', sheet_name='registeredDeathsRegions')\n",
    "\n",
    "# taking data for the contigency table \n",
    "london_covid_deaths = deaths_region[(deaths_region.Region == 'London') & \n",
    "                                    (deaths_region.causeOfDeath == 'COVID 19')].numberOfDeaths.sum()\n",
    "london_otherCause_deaths = deaths_region[(deaths_region.Region == 'London') & \n",
    "                                    (deaths_region.causeOfDeath != 'COVID 19')].numberOfDeaths.sum()\n",
    "other_covid_deaths = deaths_region[(deaths_region.Region != 'London') & \n",
    "                                    (deaths_region.causeOfDeath == 'COVID 19')].numberOfDeaths.sum()\n",
    "other_otherCause_deaths = deaths_region[(deaths_region.Region != 'London') & \n",
    "                                    (deaths_region.causeOfDeath != 'COVID 19')].numberOfDeaths.sum()\n",
    "\n",
    "# quick check to see if the sums we get sum up to all deaths\n",
    "print(london_covid_deaths+london_otherCause_deaths+other_covid_deaths+other_otherCause_deaths)\n",
    "print(deaths_region.numberOfDeaths.sum())\n",
    "\n",
    "# ceating a contigency table\n",
    "ct = [[london_covid_deaths, london_otherCause_deaths], [other_covid_deaths, other_otherCause_deaths]]\n",
    "stat, pval, dof, expected = chi2_contingency(ct)\n",
    "print(stat, pval, dof, expected)\n",
    "\n",
    "# determine association based on p-value and significance level  \n",
    "if pval <= 0.05:\n",
    "    print('There is a difference between London and other regions in COVID-19 vs other causes of death')\n",
    "else:\n",
    "    print('There seems to be no difference between London and other regions in COVID-19 vs other causes of death')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 9</h3>\n",
    "\n",
    "Apply [$χ^2$ test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html) to find if there is a difference between care homes and hospital deaths in COVID-19 vs other causes (for England and for Wales separately)?\n",
    "<br>(dataset: `weeklyRegisteredDeathsByLocationOtherCausesPlusCOVID.xlsx`,  sheet: _registeredDeathsRegions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print these pandas commands to look at the types of values in the locationOfRegisteredDeath and Region columns\n",
    "# this will ensure we are taking the right values for the data in the contigency tables\n",
    "(deaths_region.locationOfRegisteredDeath.value_counts())\n",
    "(deaths_region.Region.value_counts())\n",
    "\n",
    "# taking data for the contigency table for England\n",
    "careHome_covid_deaths_e = deaths_region[(deaths_region.locationOfRegisteredDeath == 'Care home') & \n",
    "                                    (deaths_region.causeOfDeath == 'COVID 19') & \n",
    "                                        (deaths_region.Region != 'Wales')].numberOfDeaths.sum()\n",
    "careHome_otherCause_deaths_e = deaths_region[(deaths_region.locationOfRegisteredDeath == 'Care home') & \n",
    "                                    (deaths_region.causeOfDeath != 'COVID 19') &\n",
    "                                    (deaths_region.Region != 'Wales')].numberOfDeaths.sum()\n",
    "hosp_covid_deaths_e = deaths_region[(deaths_region.locationOfRegisteredDeath == 'Hospital') & \n",
    "                                    (deaths_region.causeOfDeath == 'COVID 19') &\n",
    "                                    (deaths_region.Region != 'Wales')].numberOfDeaths.sum()\n",
    "hosp_otherCause_deaths_e = deaths_region[(deaths_region.locationOfRegisteredDeath == 'Hospital') & \n",
    "                                    (deaths_region.causeOfDeath != 'COVID 19') &\n",
    "                                    (deaths_region.Region != 'Wales')].numberOfDeaths.sum()\n",
    "\n",
    "# taking data for the contigency table for Wales\n",
    "careHome_covid_deaths_w = deaths_region[(deaths_region.locationOfRegisteredDeath == 'Care home') & \n",
    "                                    (deaths_region.causeOfDeath == 'COVID 19') & \n",
    "                                        (deaths_region.Region == 'Wales')].numberOfDeaths.sum()\n",
    "careHome_otherCause_deaths_w = deaths_region[(deaths_region.locationOfRegisteredDeath == 'Care home') & \n",
    "                                    (deaths_region.causeOfDeath != 'COVID 19') &\n",
    "                                    (deaths_region.Region == 'Wales')].numberOfDeaths.sum()\n",
    "hosp_covid_deaths_w = deaths_region[(deaths_region.locationOfRegisteredDeath == 'Hospital') & \n",
    "                                    (deaths_region.causeOfDeath == 'COVID 19') &\n",
    "                                    (deaths_region.Region == 'Wales')].numberOfDeaths.sum()\n",
    "hosp_otherCause_deaths_w = deaths_region[(deaths_region.locationOfRegisteredDeath == 'Hospital') & \n",
    "                                    (deaths_region.causeOfDeath != 'COVID 19') &\n",
    "                                    (deaths_region.Region == 'Wales')].numberOfDeaths.sum()\n",
    "\n",
    "# quick check to see if the sums we get sum up to all deaths\n",
    "all_others = deaths_region[(deaths_region.locationOfRegisteredDeath != 'Hospital') & \n",
    "                                    (deaths_region.locationOfRegisteredDeath != 'Care home')].numberOfDeaths.sum()\n",
    "print(careHome_covid_deaths_e + careHome_covid_deaths_w + careHome_otherCause_deaths_e + careHome_otherCause_deaths_w\n",
    "     + hosp_covid_deaths_e + hosp_covid_deaths_w + hosp_otherCause_deaths_e + hosp_otherCause_deaths_w + all_others)\n",
    "print(deaths_region.numberOfDeaths.sum())\n",
    "\n",
    "# ceating a contigency table for England and Wales\n",
    "ct_e = [[careHome_covid_deaths_e, careHome_otherCause_deaths_e], [hosp_covid_deaths_e, hosp_otherCause_deaths_e]]\n",
    "ct_w = [[careHome_covid_deaths_w, careHome_otherCause_deaths_w], [hosp_covid_deaths_w, hosp_otherCause_deaths_w]]\n",
    "\n",
    "stat_e, pval_e, dof_e, expected_e = chi2_contingency(ct_e)\n",
    "stat_w, pval_w, dof_w, expected_w = chi2_contingency(ct_w)\n",
    "\n",
    "# determine association based on p-value and significance level  \n",
    "if pval_e <= 0.05:\n",
    "    print('There is a difference between care home and hospital deaths in COVID-19 vs other causes in England')\n",
    "else:\n",
    "    print('There seems to be no difference between care home and hospital deaths in COVID-19 vs other causes in England')\n",
    "    \n",
    "if pval_w <= 0.05:\n",
    "    print('There is a difference between care home and hospital deaths in COVID-19 vs other causes in Wales')\n",
    "else:\n",
    "    print('There seems to be no difference between care home and hospital deaths in COVID-19 vs other causes in Wales')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extra Assignment G </h3>\n",
    "\n",
    "Apply $χ^2$ tests (and adjust for multiple testing if applicable) to find if there are differences between Westminster and other local authorities in terms of COVID-19 vs other causes? Are there authorities with significantly higher/lower COVID-19-related deaths?\n",
    "<br>(dataset: `weeklyRegisteredDeathsByLocationOtherCausesPlusCOVID.xlsx`,  sheet: _registeredDeathsLocalAuthority_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_local = pd.read_excel('weeklyRegisteredDeathsByLocationOtherCausesPlusCOVID.xlsx', sheet_name='registeredDeathsLocalAuthority')\n",
    "\n",
    "westminster_covid_deaths = deaths_local[(deaths_local.areaName == 'Westminster') & \n",
    "                                (deaths_local.causeOfDeath == 'COVID 19')].numberOfDeaths.sum()\n",
    "\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 10</h3>\n",
    "\n",
    "Calculate the pairwise correlations (what [correlation](https://docs.scipy.org/doc/scipy/reference/stats.html#correlation-functions) will you use) between all items and cluster the data (what metrics), what products have similar price changes as toilet paper? And what about rice? Are there other clusters that (don’t) surprise you? What item is least similar to other items?\n",
    "<br>(dataset: `highDemandProducts.xlsx`,  sheet: _highDemandProductPrices1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_excel('highDemandProducts.xlsx', sheet_name='highDemandProductPrices1')\n",
    "\n",
    "# look at how the product data are distributed\n",
    "for i in products.columns[1:]:\n",
    "    column_name = str(i)\n",
    "    if scipy.stats.shapiro(products[column_name].dropna())[1] > 0.05:\n",
    "        print('normally distributed:', column_name, shapiro(products[column_name].dropna())[1])\n",
    "    else:\n",
    "        print('not normally distributed:', column_name, shapiro(products[column_name].dropna())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the documentation for different options for [correlation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html), [distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html), [linkage](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) and [cluster](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html) methods used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = products.iloc[:, 1:].corr('spearman') # calculating spearman pairwise correlations using a pandas function\n",
    "pdist = sch.distance.pdist(corr_df, 'correlation') # calculating pairwise distances needeed for next step\n",
    "linkage = sch.linkage(pdist, method='average') # perform hierarchical/agglomerative clustering\n",
    "group_ids = sch.fcluster(linkage, 0.5 * pdist.max(), 'distance') # form flat clusters from the hierarchical clustering defined by the given linkage matrix\n",
    "print(group_ids)\n",
    "\n",
    "# group the products based on clusters (for visualisation)\n",
    "group1 = []\n",
    "group2 = []\n",
    "group3 = []\n",
    "group4 = []\n",
    "group5 = []\n",
    "for i, c in zip(group_ids, corr_df.columns): \n",
    "    if i == 1:\n",
    "        group1.append(c)\n",
    "    if i == 2:\n",
    "        group2.append(c)\n",
    "    if i == 3:\n",
    "        group3.append(c)\n",
    "    if i == 4:\n",
    "        group4.append(c)\n",
    "    if i == 5:\n",
    "        group5.append(c)\n",
    "cols_ordered = group1 + group2 + group3 + group4 + group5\n",
    "print(group1, group2, group3, group4, group5)\n",
    "\n",
    "sns_df = products.iloc[:, 1:][cols_ordered].corr('spearman') # re-ordering the columns for easier visualisation in heatmap\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(sns_df, annot=False, cmap=\"RdYlBu_r\", center=0) # note we changed, reversed and centered the colourmap (remove it and see what happens with defaults)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assignment 11</h3>\n",
    "\n",
    "Calculate a regression model to impute missing values for toilet paper using your selected variable(s), how close did your model get to the actual numbers? How about for rice? \n",
    "<br>(dataset: `highDemandProducts.xlsx`,  sheets: _highDemandProductPrices1_ and _highDemandProductPrices2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from above it looks like the variable correlating the most with toilet paper is tissues\n",
    "products.iloc[:, 20:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = products['Kitchen rolls']\n",
    "y = products['Toilet rolls']\n",
    "\n",
    "mask = ~np.isnan(x) & ~np.isnan(y) # remove NaN values to make both arrays same size\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x[mask], y[mask])\n",
    "print(\"r-squared:\", r_value**2)\n",
    "sns.scatterplot(x, y)\n",
    "sns.lineplot(x, intercept+slope*x, color='r')\n",
    "plt.show()\n",
    "\n",
    "# solve for the response variable (toilet rolls) \n",
    "y1 = intercept + slope*x[2]\n",
    "y2 = intercept + slope*x[7]\n",
    "y3 = intercept + slope*x[15]\n",
    "\n",
    "# compare with actual numbers\n",
    "print(y1, y2, y3)\n",
    "products_nonan = pd.read_excel('highDemandProducts.xlsx', sheet_name='highDemandProductPrices2')\n",
    "products_nonan.iloc[[2,7,15], 20:22]\n",
    "\n",
    "\n",
    "# what if you add more than one variable for the imputation? \n",
    "# another product that correlates with toilet rolls is kithen rolls...\n",
    "# hint: you will have to use a different function for multivariable regression (OLS from statsmodels.api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extra Assignments H & I </h3>\n",
    "\n",
    "Instead of using the correlations and hierarchical clustering (from assignment 10) use stepwise regression (backward elimination or forward addition), how does this compare to the results from assignment 11? \n",
    "\n",
    "Now apply your favourite ML/DL method to do the same as in extra assignment H, how does it do? \n",
    "<br>(dataset: `highDemandProducts.xlsx`,  sheets: _highDemandProductPrices1_ and _highDemandProductPrices2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extra Assignments J & K & L & M </h3>\n",
    "\n",
    "Are there differences in the slopes of men and women (<1 vs 90+) between the 4 countries in the UK in 2001? What do you observe? \n",
    "\n",
    "Are these differences in slope persistent over the years 2002-2019?\n",
    "\n",
    "How has the difference in life expectancy at birth between men and women changed from 2001-2019? What about for other age groups? (reminder: paired data of each local authority) \n",
    "\n",
    "What happens if you remove outliers and redo the example or extra assignment J? You can use your own definition, or use: exclude data from any local authority for which either men or women are in the lower or upper 0.5 percentiles. \n",
    "<br>(dataset: `lifeExpectancyUK.xlsx`,  sheets: _lifeExpectancy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy = pd.read_excel('lifeExpectancyUK.xlsx')\n",
    "# your code goes here\n",
    "sc = life_expectancy[life_expectancy.Country == 'Scotland']\n",
    "en = life_expectancy[life_expectancy.Country == 'England']\n",
    "wa = life_expectancy[life_expectancy.Country == 'Wales']\n",
    "ni = life_expectancy[life_expectancy.Country == 'Northern Ireland']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope_intercept(df, sex, year): # this function will return the slope and intercept (in that order) \n",
    "     return linregress(x=df[(df['Sex'] == sex) & (df['Age group'] == \"'90+'\")][year], \n",
    "           y=df[(df['Sex'] == sex) & (df['Age group'] == \"'<1'\")][year])[:2]\n",
    "print('Scotland, males:', 'slope:',slope_intercept(sc, 'Male', 2001)[0], 'intercept:',slope_intercept(sc, 'Male', 2001)[1])\n",
    "print(slope_intercept(sc, 'Female', 2001))\n",
    "print(slope_intercept(en, 'Male', 2001))\n",
    "print(slope_intercept(en, 'Female', 2001))\n",
    "print(slope_intercept(wa, 'Male', 2001))\n",
    "print(slope_intercept(wa, 'Female', 2001))\n",
    "print(slope_intercept(ni, 'Male', 2001))\n",
    "print(slope_intercept(ni, 'Female', 2001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## interaction model for Scotland\n",
    "\n",
    "# first create the x variables\n",
    "x = sc[(sc['Age group'] == \"'90+'\")] # the LE at 90+ for both men and wome in Scotland\n",
    "x = x[['Sex', 2001]]\n",
    "x['Sex'] = x['Sex'].map({'Female': 1, 'Male': 0})\n",
    "x['interaction'] = x['Sex']*x[2001]\n",
    "\n",
    "# sex column refers to the sex (1 for women, 0 for men)\n",
    "# 2001 column is the LE at 90+ for both men and wome in Scotland\n",
    "# interaction column is the product of first two\n",
    "# print the x dataframe to look at the variables\n",
    "\n",
    "# the y variable is the LE at <1 for both men and women\n",
    "y = sc[(sc['Age group'] == \"'<1'\")][2001]\n",
    "\n",
    "# resetting the indices\n",
    "x = x.reset_index().drop(['index'], axis=1)\n",
    "y = y.reset_index().drop(['index'], axis=1)\n",
    "x = add_constant(x)\n",
    "model = OLS(endog=y, exog=x).fit()\n",
    "\n",
    "# print out the p-values for the variables\n",
    "print(model.pvalues)\n",
    "\n",
    "# can also print a complete model summary (check out the documentation) -- for some reason it does not include pvals\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same steps for the rest of the UK countries (England, Wales, Northern Ireland).\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
